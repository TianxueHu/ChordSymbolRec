{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "from collections import Counter\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, Subset, random_split\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from chord_rec.models.lit_seq2seq import LitSeq2Seq\n",
    "\n",
    "from chord_rec.datasets.vec_datasets import Vec45Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointEveryNEpoch(pl.Callback):\n",
    "    def __init__(self, start_epoc, ckpt_every_n = 1):\n",
    "        self.start_epoc = start_epoc\n",
    "        self.ckpt_every_n = ckpt_every_n\n",
    "\n",
    "    def on_epoch_end(self, trainer: pl.Trainer, _):\n",
    "        \"\"\" Check if we should save a checkpoint after every train epoch \"\"\"\n",
    "        # file_path = f\"{trainer.logger.log_dir}/checkpoints/epoch={trainer.current_epoch}.pt\"\n",
    "        epoch = trainer.current_epoch\n",
    "        if epoch >= self.start_epoc and epoch % self.ckpt_every_n == 0:\n",
    "            ckpt_path = f\"{trainer.logger.log_dir}/checkpoints/epoch={epoch}.ckpt\"\n",
    "            trainer.save_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_dir = \"D:\\\\Documents\\\\2021Spring\\\\ChordSymbolRec\\\\chord_rec\\\\logs\\\\final_run\\\\version_0\"\n",
    "hparams_path = os.path.join(ckp_dir, \"hparams.yaml\")\n",
    "checkpoint_path = os.path.join(ckp_dir, \"checkpoints\", \"epoch=157-step=17379.ckpt\")\n",
    "\n",
    "all_conf = OmegaConf.load(hparams_path)\n",
    "conf = all_conf.configs\n",
    "data_conf = conf.dataset\n",
    "seed = conf.experiment.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if conf.experiment.device == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "data_root = conf.dataset.directory\n",
    "dataset_name = conf.dataset.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open(data_conf.fpath, \"rb\"))\n",
    "\n",
    "note_seq, chord_seq = [],[]\n",
    "max_seq_len = 0\n",
    "data_num = 0\n",
    "for file in data:\n",
    "    data_num += len(file)\n",
    "    for window in file:\n",
    "        note_seq.append(window[0])\n",
    "        chord_seq.append(window[1])\n",
    "        max_seq_len = max(max_seq_len, len(window[1]))\n",
    "\n",
    "note_padding_vec = np.full(len(note_seq[0][0]), -1).reshape(1,-1) # should be 45; not sure if -1 is good\n",
    "note_ending_vec = np.ones(len(note_seq[0][0])).reshape(1,-1) # should be 45\n",
    "note_starting_vec = np.zeros(len(note_seq[0][0])).reshape(1,-1) # should be 45\n",
    "\n",
    "chord_start = \"<sos>\"\n",
    "chord_padding = \"<pad>\"\n",
    "chord_end = \"<eos>\"\n",
    "\n",
    "padded_note_seq = []\n",
    "padded_chord_seq = []\n",
    "\n",
    "eval_masks = []\n",
    "\n",
    "for i in range(len(note_seq)):\n",
    "    len_diff = max_seq_len - len(note_seq[i])\n",
    "\n",
    "    temp_note_vec = np.vstack((note_starting_vec, np.array(note_seq[i]), note_ending_vec, np.repeat(note_padding_vec, len_diff , axis = 0)))\n",
    "    padded_note_seq.append(temp_note_vec)\n",
    "\n",
    "    eval_masks.append([False] + [True for _ in range(len(note_seq[i]))] + [False for _ in range(len_diff+1)])\n",
    "    temp_chord_vec = np.hstack((chord_start, np.array(chord_seq[i]), chord_end, np.repeat(chord_padding, len_diff , axis = 0)))\n",
    "    padded_chord_seq.append(temp_chord_vec)\n",
    "\n",
    "eval_masks = np.array(eval_masks)\n",
    "stacked_note_seq = np.stack(padded_note_seq, axis = 0)\n",
    "stacked_chord_seq = np.vstack(padded_chord_seq)\n",
    "\n",
    "note_vec = np.asarray(stacked_note_seq, dtype = np.float32)\n",
    "chord_vocab = Vocab(Counter(list(stacked_chord_seq.flatten())))\n",
    "\n",
    "vec_size = note_vec.shape[-1]\n",
    "vocab_size = len(chord_vocab.stoi)\n",
    "\n",
    "assert data_conf.val_ratio + data_conf.test_ratio <= 0.6, \"At least 40 percent of the data needed for training\"\n",
    "\n",
    "dataset = Vec45Dataset(note_vec, stacked_chord_seq, eval_masks, chord_vocab)\n",
    "\n",
    "\n",
    "train_ratio = 1 - data_conf.val_ratio - data_conf.test_ratio\n",
    "\n",
    "train_len = int(len(dataset)*train_ratio)\n",
    "val_len = int(len(dataset)*data_conf.val_ratio)\n",
    "test_len = len(dataset) - train_len - val_len\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len], \n",
    "                                                generator=torch.Generator().manual_seed(seed)\n",
    "                                               )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size =data_conf.batch_size, shuffle = data_conf.shuffle_train, num_workers = data_conf.num_workers, drop_last = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = data_conf.batch_size, shuffle = data_conf.shuffle_val, num_workers = data_conf.num_workers, drop_last = True)\n",
    "test_loader =  DataLoader(test_dataset, batch_size = data_conf.batch_size, shuffle = data_conf.shuffle_val, num_workers = data_conf.num_workers, drop_last = True)\n",
    "\n",
    "MAX_LEN = max_seq_len + 2\n",
    "\n",
    "if conf.model.type == \"attn_s2s\":\n",
    "    model = LitSeq2Seq(vec_size, MAX_LEN, chord_vocab, conf)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x281a22dfc08>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitSeq2Seq.load_from_checkpoint(checkpoint_path, chord_vocab = chord_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "INFO:lightning:GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
      "D:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "epochs = conf.training.warm_up + conf.training.decay_run + conf.training.post_run\n",
    "tb_logger = pl_loggers.TensorBoardLogger(conf.logging.output_dir, name = conf.experiment.objective)\n",
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47c44964f5e4ecf8332c44e8b6d389f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▎                                                                       | 1573/27491 [00:08<02:36, 165.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 27491/27491 [02:30<00:00, 182.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.7005842924118042,\n",
      " 'test_name_acc': 0.8738859990542359,\n",
      " 'test_quality_acc': 0.8848714124622604,\n",
      " 'test_root_acc': 0.9303408388199774,\n",
      " 'test_similarity': 0.9357801947303966}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7005842924118042,\n",
       "  'test_name_acc': 0.8738859990542359,\n",
       "  'test_root_acc': 0.9303408388199774,\n",
       "  'test_quality_acc': 0.8848714124622604,\n",
       "  'test_similarity': 0.9357801947303966}]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c4b2f17dc4d72aed2bd8742bb0748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_pred2 = []\n",
    "all_label2 = []\n",
    "eval_masks = []\n",
    "for idx, (note, chord, mask) in enumerate(tqdm(test_loader)):\n",
    "        pred = model(note.to(device), chord.long().to(device), teacher_forcing = False, start_idx = chord_vocab.stoi[\"<sos>\"])\n",
    "        pred = pred.detach().cpu().numpy().argmax(axis = -1)\n",
    "        \n",
    "        label = chord.detach().cpu().numpy()\n",
    "        pred[:,0] = np.full(len(pred), chord_vocab.stoi[\"<sos>\"])\n",
    "        all_pred2.append(dataset.vec_decode(pred))\n",
    "        all_label2.append(dataset.vec_decode(label))\n",
    "        eval_masks.append(mask.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred2 = np.vstack(all_pred2)\n",
    "all_label2 = np.vstack(all_label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<sos>', 'B dominant seventh', 'E major', 'E major',\n",
       "       'Cx german augmented sixth', 'Cx german augmented sixth',\n",
       "       'Cx german augmented sixth', 'Cx german augmented sixth',\n",
       "       'Cx german augmented sixth', 'Cx german augmented sixth',\n",
       "       'D# major', 'D# major', 'D# major', 'D# major',\n",
       "       'Cx italian augmented sixth', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>', '<eos>',\n",
       "       '<eos>', '<eos>', '<eos>'], dtype='<U27')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<sos>', 'B major seventh', 'E major', 'E major', 'E major',\n",
       "       'Cx german augmented sixth', 'Cx german augmented sixth',\n",
       "       'Cx german augmented sixth', 'Cx german augmented sixth',\n",
       "       'Cx german augmented sixth', 'C# minor', 'D# major', 'D# major',\n",
       "       'D# major', 'Cx italian augmented sixth', '<eos>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U27')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds = all_pred2\n",
    "decoded_chords = all_label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['<sos>', 'B dominant seventh', 'E major', ..., '<eos>', '<eos>',\n",
       "        '<eos>'],\n",
       "       ['<sos>', 'A# major', 'A# major', ..., '<eos>', '<eos>', '<eos>'],\n",
       "       ['<sos>', 'Bb major', 'Bb major', ..., '<eos>', '<eos>', '<eos>'],\n",
       "       ...,\n",
       "       ['<sos>', 'G major', 'D major', ..., '<eos>', '<eos>', '<eos>'],\n",
       "       ['<sos>', 'F major', 'F major', ..., '<eos>', '<eos>', '<eos>'],\n",
       "       ['<sos>', 'B major', 'B major', ..., '<eos>', '<eos>', '<eos>']],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_masks = np.vstack(eval_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8727689628173785\n"
     ]
    }
   ],
   "source": [
    "# mask = (decoded_preds != \"<sos>\") & (decoded_preds != \"<eos>\") & (decoded_preds != \"<pad>\")\n",
    "mask = eval_masks\n",
    "masked_preds = decoded_preds[mask]\n",
    "masked_chords = decoded_chords[mask]\n",
    "\n",
    "print(np.sum(masked_preds == masked_chords) / len(masked_chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump({\"preds\":all_pred2, \"labels\": all_label2 }, open(\"examples/output/haydn_red1_preds.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True, False,  True])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_preds == masked_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPERATE EVALUATION OF ROOT AND QUALITY AFTER DECODING\n",
    "# seperate all pred \n",
    "root_preds = decoded_preds.copy()\n",
    "quality_preds = decoded_preds.copy()\n",
    "for r_id in range(decoded_preds.shape[0]):\n",
    "    for c_id in range(decoded_preds.shape[1]):\n",
    "        sp = decoded_preds[r_id, c_id].split(' ')\n",
    "        root_preds[r_id, c_id] = sp[0]\n",
    "        quality_preds[r_id, c_id] = ' '.join(sp[1:])\n",
    "    \n",
    "root_labels = decoded_chords.copy()\n",
    "quality_labels = decoded_chords.copy()\n",
    "for r_id in range(decoded_chords.shape[0]):\n",
    "    for c_id in range(decoded_chords.shape[1]):\n",
    "        sp = decoded_chords[r_id, c_id].split(' ')\n",
    "        root_labels[r_id, c_id] = sp[0]\n",
    "        quality_labels[r_id, c_id] = ' '.join(sp[1:])\n",
    "# # seperate all lable \n",
    "# root_labels = []\n",
    "# quality_labels = []\n",
    "# for c in decoded_chords:\n",
    "#     sp = c.split(' ')\n",
    "#     root_labels.append(sp[0])\n",
    "#     quality_labels.append(' '.join(sp[1:]))\n",
    "    \n",
    "# root_labels = np.asarray(root_labels)\n",
    "# quality_labels = np.asarray(quality_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (root_preds != \"<sos>\") & (root_preds != \"<eos>\") & (root_preds != \"<pad>\")\n",
    "root_preds = root_preds[mask]\n",
    "quality_preds = quality_preds[mask]\n",
    "root_label = root_labels[mask]\n",
    "quality_labels = quality_labels[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8458368083952097"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(root_preds == root_label) / len(root_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411080251661956"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(quality_preds == quality_labels) / len(quality_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0050122737884521484\n",
      "0.0039865970611572266\n",
      "0.004013538360595703\n",
      "0.004997968673706055\n",
      "0.005002021789550781\n",
      "0.004987001419067383\n",
      "0.005013465881347656\n",
      "0.0049877166748046875\n",
      "0.004999399185180664\n",
      "0.003989696502685547\n",
      "0.004014253616333008\n",
      "0.004986763000488281\n",
      "0.0040132999420166016\n",
      "0.003999948501586914\n",
      "0.005000114440917969\n",
      "0.004999637603759766\n",
      "0.004986763000488281\n",
      "0.005013465881347656\n",
      "0.005002498626708984\n",
      "0.004984617233276367\n",
      "0.005012989044189453\n",
      "0.0039861202239990234\n",
      "0.005013704299926758\n",
      "0.0049855709075927734\n",
      "0.005014657974243164\n",
      "0.00398564338684082\n",
      "0.0050008296966552734\n",
      "0.0040132999420166016\n",
      "0.004000186920166016\n",
      "0.004999637603759766\n",
      "0.004999876022338867\n",
      "0.004000186920166016\n",
      "0.0039865970611572266\n",
      "0.0040132999420166016\n",
      "0.004999876022338867\n",
      "0.00500035285949707\n",
      "0.005000114440917969\n",
      "0.004986286163330078\n",
      "0.005013465881347656\n",
      "0.004000186920166016\n",
      "0.004999637603759766\n",
      "0.005013942718505859\n",
      "0.005000114440917969\n",
      "0.004000186920166016\n",
      "0.0049855709075927734\n",
      "0.004014015197753906\n",
      "0.005000114440917969\n",
      "0.004000425338745117\n",
      "0.004928112030029297\n",
      "0.005000591278076172\n",
      "0.004999876022338867\n",
      "0.004999637603759766\n",
      "0.003999948501586914\n",
      "0.003999233245849609\n",
      "0.004000663757324219\n",
      "0.00398564338684082\n",
      "0.004014492034912109\n",
      "0.004999876022338867\n",
      "0.004000186920166016\n",
      "0.004000186920166016\n",
      "0.003997325897216797\n",
      "0.00400233268737793\n",
      "0.004003286361694336\n",
      "0.004000663757324219\n",
      "0.004010438919067383\n",
      "0.004999637603759766\n",
      "0.0039861202239990234\n",
      "0.02201390266418457\n",
      "0.0409998893737793\n",
      "0.04098677635192871\n",
      "0.00500178337097168\n",
      "0.005013704299926758\n",
      "0.004997968673706055\n",
      "0.004000186920166016\n",
      "0.0049855709075927734\n",
      "0.004014015197753906\n",
      "0.004013538360595703\n",
      "0.004999637603759766\n",
      "0.004000186920166016\n",
      "0.003988504409790039\n",
      "0.005011796951293945\n",
      "0.003999471664428711\n",
      "0.004000186920166016\n",
      "0.004987001419067383\n",
      "0.0040132999420166016\n",
      "0.004000186920166016\n",
      "0.004999399185180664\n",
      "0.005000114440917969\n",
      "0.003999948501586914\n",
      "0.00500035285949707\n",
      "0.003997325897216797\n",
      "0.004013538360595703\n",
      "0.003999233245849609\n",
      "0.003988742828369141\n",
      "0.004000425338745117\n",
      "0.004010915756225586\n",
      "0.004002809524536133\n",
      "0.004999876022338867\n",
      "0.00398564338684082\n",
      "0.004014253616333008\n"
     ]
    }
   ],
   "source": [
    "# for i in range(100):\n",
    "\n",
    "#     start = time.time()\n",
    "#     avg_similarity = chord_similarity(masked_preds[i], masked_chords[i])\n",
    "#     end = time.time()\n",
    "#     print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "print(len(masked_chords) // 1000 * 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.800968408584595"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = Counter(chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_chord = list(list(zip(*co.most_common(20)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(decoded_preds)):\n",
    "    if decoded_preds[i] not in most_common_chord:\n",
    "        decoded_preds[i] = \"others\"\n",
    "    if decoded_chords[i] not in most_common_chord:\n",
    "        decoded_chords[i] = \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-74e52cbe44c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_chords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_common_chord\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"others\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(decoded_chords, decoded_preds, normalize = \"true\", labels = most_common_chord + [\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,10)) \n",
    "\n",
    "sn.heatmap(cm, annot=False)\n",
    "ax.set_xticklabels(most_common_chord + [\"others\"])\n",
    "ax.set_yticklabels(most_common_chord + [\"others\"])\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=\"vertical\") \n",
    "plt.show()\n",
    "fig.savefig(\"confusion_bachhaydn_baseline.pdf\", format = \"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"baseline_bach_and_haydn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol, num = list(zip(*co.most_common(50)))\n",
    "symbol = list(symbol)\n",
    "num = list(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol += ['others']\n",
    "num += [np.sum(list(co.values())) - np.sum(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num/=np.sum(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(13,10)) \n",
    "x_pos = [i for i, _ in enumerate(symbol)]\n",
    "\n",
    "plt.bar(x_pos, num)\n",
    "plt.xlabel(\"Chord Symbol\")\n",
    "plt.ylabel(\"Occurance\")\n",
    "plt.title(\"bach chorales Chord Distribution\")\n",
    "\n",
    "plt.xticks(x_pos, symbol, rotation = \"vertical\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
