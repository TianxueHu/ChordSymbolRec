{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from chord_rec.models.lit_baseline import LitBaseline\n",
    "from chord_rec.datasets.vec_datasets import FFNNDataset\n",
    "\n",
    "import random\n",
    "import os\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointEveryNEpoch(pl.Callback):\n",
    "    def __init__(self, start_epoc, ckpt_every_n = 1):\n",
    "        self.start_epoc = start_epoc\n",
    "        self.ckpt_every_n = ckpt_every_n\n",
    "\n",
    "    def on_epoch_end(self, trainer: pl.Trainer, _):\n",
    "        \"\"\" Check if we should save a checkpoint after every train epoch \"\"\"\n",
    "        # file_path = f\"{trainer.logger.log_dir}/checkpoints/epoch={trainer.current_epoch}.pt\"\n",
    "        epoch = trainer.current_epoch\n",
    "        if epoch >= self.start_epoc and epoch % self.ckpt_every_n == 0:\n",
    "            ckpt_path = f\"{trainer.logger.log_dir}/checkpoints/epoch={epoch}.ckpt\"\n",
    "            trainer.save_checkpoint(ckpt_path)\n",
    "            \n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_acc',\n",
    "   min_delta=0.00,\n",
    "   patience=10,\n",
    "   verbose=True,\n",
    "   mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_dir = \"D:\\\\Documents\\\\2021Spring\\\\ChordSymbolRec\\\\chord_rec\\\\logs\\\\FFNN\\\\version_5\"\n",
    "hparams_path = os.path.join(ckp_dir, \"hparams.yaml\")\n",
    "checkpoint_path = os.path.join(ckp_dir, \"checkpoints\", \"epoch=3-step=15075.ckpt\")\n",
    "\n",
    "all_conf = OmegaConf.load(hparams_path)\n",
    "conf = all_conf.configs\n",
    "data_conf = conf.dataset\n",
    "seed = conf.experiment.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if conf.experiment.device == \"gpu\" and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "data_root = conf.dataset.directory\n",
    "dataset_name = conf.dataset.name\n",
    "\n",
    "train_path = os.path.join(data_root, conf.dataset.train_fname)\n",
    "val_path = os.path.join(data_root, conf.dataset.val_fname)\n",
    "test_path = os.path.join(data_root, conf.dataset.test_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pkl.load(open(train_path,\"rb\"))\n",
    "train = [np.array(x) for x in train]\n",
    "train = np.vstack(train)\n",
    "train_notes, train_chords = train[:, :-1], train[:,-1]\n",
    "\n",
    "val = pkl.load(open(val_path,\"rb\"))\n",
    "val = [np.array(x) for x in val]\n",
    "val = np.vstack(val)\n",
    "val_notes, val_chords = val[:, :-1], val[:,-1]\n",
    "\n",
    "test = pkl.load(open(test_path,\"rb\"))\n",
    "test = [np.array(x) for x in test]\n",
    "test = np.vstack(test)\n",
    "test_notes, test_chords = test[:, :-1], test[:,-1]\n",
    "\n",
    "\n",
    "chords = np.hstack([train_chords, val_chords, test_chords])\n",
    "chord_vocab = Vocab(Counter(chords))\n",
    "\n",
    "# encoded_train_chords = [chord_vocab.stoi[ch] for ch in train_chords]\n",
    "# encoded_val_chords = [chord_vocab.stoi[ch] for ch in val_chords]\n",
    "encoded_test_chords = [chord_vocab.stoi[ch] for ch in test_chords]\n",
    "\n",
    "# train_dataset = FFNNDataset(train_notes, encoded_train_chords)\n",
    "# val_dataset = FFNNDataset(val_notes, encoded_val_chords)\n",
    "test_dataset = FFNNDataset(test_notes, encoded_test_chords)\n",
    "\n",
    "vec_size = len(train_notes[0])\n",
    "vocab_size = vocab_size = len(chord_vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size =data_conf.batch_size, shuffle = data_conf.shuffle_train, num_workers = data_conf.num_workers, drop_last = True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = data_conf.batch_size, shuffle = data_conf.shuffle_val, num_workers = data_conf.num_workers, drop_last = True)\n",
    "test_loader =  DataLoader(test_dataset, batch_size = data_conf.batch_size, shuffle = data_conf.shuffle_val, num_workers = data_conf.num_workers, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a22542c804489d9f99a9225a49cd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 120608/120608 [09:18<00:00, 216.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.5321123003959656,\n",
      " 'test_loss': 1.8942837715148926,\n",
      " 'test_name_acc': 0.5321122976916954,\n",
      " 'test_quality_acc': 0.6621285486866543,\n",
      " 'test_root_acc': 0.6294773149376492,\n",
      " 'test_similarity': 0.6905683426196161}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.8942837715148926,\n",
       "  'test_acc': 0.5321123003959656,\n",
       "  'test_name_acc': 0.5321122976916954,\n",
       "  'test_root_acc': 0.6294773149376492,\n",
       "  'test_quality_acc': 0.6621285486866543,\n",
       "  'test_similarity': 0.6905683426196161}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LitBaseline.load_from_checkpoint(checkpoint_path, chord_vocab = chord_vocab)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(model, test_dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
